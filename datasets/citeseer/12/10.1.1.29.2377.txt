aaai workshop arti cial intelligence web search july impact similarity measures web page clustering alexander strehl joydeep ghosh raymond mooney university texas austin austin tx usa email strehl ece utexas edu ghosh ece utexas edu mooney cs utexas edu clustering web documents enables semi automated categorization facilitates certain types search 
clustering method embed documents suitable similarity space 
clustering methods associated similarity measures proposed past systematic comparative study impact similarity metrics cluster quality possibly popular cost criteria readily translate qualitatively di erent metrics 
observe domains yahoo provide categorization human experts useful criteria comparisons similarity metrics available 
compare popular similarity measures euclidean cosine pearson correlation extended jaccard conjunction clustering techniques random self organizing feature map hyper graph partitioning generalized means weighted graph partitioning high dimensional sparse data representing web documents 
performance measured human imposed classi cation news categories industry categories 
conduct number experiments tests assure statistical signi cance results 
cosine extended jaccard similarities emerge best measures capture human categorization behavior euclidean performs poorest 
weighted graph partitioning approaches clearly superior 
increasing size dynamic content world wide web created need automated organization web pages 
document clusters provide structure organizing large bodies text ecient browsing searching 
purpose web page typically represented vector consisting suitably normalized frequency counts words terms 
document contains small percentage words web 
consider document multi dimensional vector try cluster documents word contents problem di ers classic clustering scenarios ways 
document clustering data high dimen copyright american association arti cial intelligence www aaai org 
rights reserved 
sional characterized highly sparse word document matrix positive ordinal attribute values signi cant amount outliers 
clustering widely studied disciplines specially early hartigan 
classic approaches include partitional methods means hierarchical agglomerative clustering unsupervised bayes soft statistical mechanics techniques 
classical techniques fairly ones proposed data mining community clarans dbscan birch clique cure 
rastogi shim distances samples original vector space 
faced curse dimensionality associated sparsity issues dealing high dimensional data 
performance clustering algorithms demonstrated illustrative dimensional examples 
documents represented bag words resulting document word matrix typically represents data dimensions 
noteworthy attempts emerged eciently cluster documents represented high dimensional space dhillon modha authors spherical means algorithm document clustering 
graphbased clustering approaches attempt avoid curse dimensionality transforming problem formulation include karypis han kumar boley strehl ghosh 
note methods variety similarity distance measures literature unaware solid comparative study di erent similarity measures 
rst compare similarity measures analytically illustrate semantics geometrically 
secondly propose experimental methodology compare high dimensional clusterings mutual information entropy purity 
conduct series experiments yahoo news pages evaluate performance cluster quality simi substantial categorizing documents 
documents labels variety supervised semi supervised techniques mooney roy yang aaai workshop arti cial intelligence web search july overview similarity clustering framework :10.1.1.16.4348
larity measures euclidean cosine pearson correlation extended jaccard combination algorithms random self organizing feature map hyper graph partitioning generalized means weighted graph partitioning number objects web pages data number features words terms sample ng 
input data represented word document matrix th column representing sample hard clustering assigns label dimensional sample similar samples tend get label 
number distinct labels desired number clusters 
general labels treated nominals inherent order cases self organizing feature maps top recursive graph bisection labeling may contain extra ordering information 
denote set objects th cluster kg jc gives overview batch clustering process set raw object descriptions vector space description similarity space description cluster labels 


kg 
section brie describes compared algorithms 
algorithms random baseline baseline comparing algorithms clustering labels drawn uniform random distribution integers complexity algorithm 
self organizing feature map dimensional sofm proposed kohonen kohonen 
generate clusters cells line topology train network epochs minutes whichever comes rst 
experiments run dual processor mhz pentium clustering technique sofm implementation matlab neural network tool box 
resulting network subsequently generate label vector index activated soft clustering record belong multiple clusters di erent degrees association kumar ghosh 
neuron sample 
complexity incremental algorithm 


determined number epochs samples generalized means employed known means algorithm variations non euclidean distance measures 
means algorithm iterative algorithm minimize squares error criterion duda hart 
cluster represented center mean samples centers initialized random selection data objects 
sample labeled index nearest similar center 
subsections describe di erent semantics closeness similarity objects subsequent re computing mean cluster re assigning cluster labels iterated convergence xed labeling iterations 
complexity algorithm 



weighted graph partitioning objects clustered viewed set vertices web pages vertices connected undirected edge positive weight cardinality set edges je equals number non zero similarities pairs samples 
set edges removal partitions graph pairwise disjoint sub graphs called edge separator 
objective nd separator minimum sum edge weights 
striving minimum cut objective number objects cluster kept approximately equal 
decided opossum strehl ghosh produces balanced equal sized clusters similarity matrix multi level multi constraint graph partitioning karypis kumar 
balanced clusters desirable cluster represents equally important share data 
natural classes may equal size 
higher number clusters account multi modal classes xor problem clusters merged stage 
expensive step 
technique computation similarity matrix 
document clustering sparsity induced looking strongest edges subgraph induced pruning edges nearest neighbors vertex 
sparsity approach feasible large data sets 
web page clustering sparsity induced non euclidean similarities proposed may increased thresholding criterion 
hyper graph partitioning hyper graph graph edges connect vertices hyper edges 
clustering problem formulated nding minimum cut aaai workshop arti cial intelligence web search july hyper graph 
minimum cut removal set hyper edges minimum edge weight separates hyper graph unconnected components 
object maps vertex word feature maps hyper edge connecting vertices non zero frequency count word 
weight hyper edge chosen total number occurrences data set 
importance hyper edge partitioning proportional occurrence corresponding word 
hyper graph unconnected components gives desired clustering 
employ hmetis package partitioning 
advantage approach clustering problem mapped graph problem explicit computation similarity approach computationally ecient 

assuming close linear performing hyper graph partitioner 
frequency information gets lost formulation weight associated hyperedge 
similarity measures metric distances minkowski distances jx standard metrics geometrical problems 
obtain manhattan euclidean distance 
euclidean space chose relate distances similarities consequently de ne euclidean normalized similarity important properties see discussion commonly adopted kx lacks 
cosine measure similarity de ned angle cosine angle vectors 
cosine measure 
kx captures scale invariant understanding similarity 
stronger property cosine similarity depend length 
allows documents composition di erent totals treated identically popular measure text documents 
due property samples normalized unit sphere ecient processing dhillon modha 
pearson correlation collaborative ltering correlation predict feature highly similar mentor group objects features known 
normalized pearson correlation de ned xa xa 
kx denotes average feature value dimensions 
extended jaccard similarity binary jaccard coecient measures ratio number shared attributes words number possessed retail market basket applications 
jaccard similarity extended continuous discrete non negative features kx strehl ghosh 
discussion clearly clusters meaningful similarity measure invariant transformations natural problem domain 
normalization may strongly ect clustering positive negative way 
features chosen carefully comparable scales similarity re ect underlying semantics task 
euclidean similarity translation invariant scale variant cosine translation variant scale invariant 
extended jaccard aspects properties illustrated gure 
iso similarity lines points shown euclidean cosine extended jaccard 
cosine similarity lines positive quadrant plotted 
dashed line marks locus equal similarity passes origin cosine extended jaccard similarity 
euclidean space iso similarities concentric hyper spheres considered sample point vector due nite range similarity radius decreases increases linearly 
radius constant similarity regardless center point 
location similarity considered point location nite distance similarity sparsity 
cosine measure renders iso similarities hyper cones having apex origin axis aligned sample vector 
locations similarity dimensional sub space de ned axis locus points similarity hyper plane perpendicular axis 
extended jaccard similarity iso similarities non concentric hyper spheres 
location point 
hyper sphere radius increases distance considered point origin longer vectors turn tolerant terms similarity smaller vectors 
sphere radius increases similarity approaches radius nite 
resulting iso similarity surface hyper plane perpendicular considered point origin 
extended jaccard behaves cosine measure behaves euclidean distance 
aaai workshop arti cial intelligence web search july euclidean cosine extended jaccard properties various similarity measures 
extended jaccard adopts middle ground euclidean cosine similarity 
traditional euclidean means clustering optimal cluster representative minimizes sum squared error sse criterion arg min kx zk proof convex objective translated extended similarity space 
consider generalized objective function cluster representative kx zk mapping distances similarities yields log log transform objective strictly monotonic decreasing function minimizing maximize similarity space squared error representative cluster satis es arg max concave evaluation function obtain optimal representatives non euclidean similarity spaces 
values evaluation function fx shade background gure 
maximum likelihood interpretation constructed distance similarity transformation 
consequently dual interpretations probabilities similarity space errors distance space 
experimental evaluation methodology conducted experiments algorithms variants means graph partitioning yielding eleven techniques total 
clustering unsupervised success generally measured cost criterion 
standard cost functions sum squared distances cluster representative depend similarity distance measure employed 
compare techniques di erent similarity measures 
situations pages categorized labelled external source plausible way 
categories classes gg true classi cation labels evaluate performance 
evaluating single cluster purity entropy entire clustering evaluated mutual information 
denote number objects cluster classi ed 
cluster purity de ned max purity interpreted classi cation rate assumption samples cluster predicted members actual dominant class cluster 
alternatively entropy de ned class problem log log entropy comprehensive measure purity just considering number objects frequent class considers entire distribution 
criteria suitable measuring single cluster quality biased favor smaller clusters 
fact criteria globally optimal value trivially reached cluster single sample 
consequently cluster wise performance evaluation measure mutual information log log 
mutual information symmetric measure degree dependency clustering categorization 
correlation mutual information takes higher order dependencies account 
symmetric mutual information criterion successfully captures related labeling categorizations bias smaller clusters 
performance measures ected distribution data priori sizes normalize performance corresponding random clustering interpret resulting ratio performance lift 
findings industry web page data yahoo industry web page data cmu web kb project craven industry sectors selected airline computer hardware electronic instruments controls forestry wood products gold silver mobile homes rvs oil services aaai workshop arti cial intelligence web search july equipment railroad software programming 
industry contributes pages 
frequencies di erent words standard english list 
occur average times page extracted html 
word location considered 
data far clean reuters data 
documents vary signi cantly length wrong category dated little content images 
hub pages yahoo refers usually top level branch pages 
tend similar bag words content di erent classes contact information search windows welcome messages news content oriented pages 
sample sizes clustering documents categories 
number clusters set setting run times technique gets data capture random variation results 
shows results experiments 
table test results indicate cosine similarity performs best closely followed non euclidean measures 
second tier non euclidean means variations 
hyper graph partitioning performs reasonably 
sofm euclidean similarity means graph partitioning fail capture relationships high dimensional data 
random sofm hgp km eucl km cosi km corr km gp eucl gp cosi gp corr gp performance lift normalized random baseline yahoo industry web page data mutual information various sample sizes bars indicate standard deviations 
findings news web pages original yahoo news categories data business entertainment sub category art cable culture film industry media multimedia music online people review stage television variety health politics sports technology correspond respectively 
data publicly available ftp ftp cs umn edu dept users boley series boley 
raw word document matrix consists occurrence frequencies stemmed words porter sux stripping algorithm frakes 
pruning words occur times average insigni cant generic new respectively results 
random sofm hgp km eucl km cosi km corr km gp eucl gp cosi gp corr gp performance lift normalized random baseline yahoo news web page data mutual information various sample sizes bars indicate standard deviations 
sample sizes clustering documents categories 
number clusters set setting run times technique gets sub sample capture random variation results 
chose clusters times number categories natural number clusters indicated preliminary runs visualization 
greater number clusters classes viewed allowing multi modal distributions classes 
example xor problem classes clusters 
clustering runs conducted gure results evaluated sided tests table sample level 
non euclidean graph partitioning approaches best data 
top performing similarity measures extended jaccard cosine 
initially expected cosine perform better extended jac aaai workshop arti cial intelligence web search july gp cosi gp corr gp km km cosi km corr hgp sofm km eucl gp eucl random gp cosi gp corr gp km km cosi km corr hgp sofm km eucl gp eucl random table industry web page data 
comparison trials techniques samples terms performance test results con dences marked 
gp gp cosi gp corr hgp km km corr km cosi sofm gp eucl random km eucl gp gp cosi gp corr hgp km km corr km cosi sofm gp eucl random km eucl table news web page data 
comparison trials techniques samples terms performance test results con dences marked 
card correlation due length invariance 
middle ground viewpoint extended jaccard successful web page market basket applications 
correlation marginally worse terms average performance 
hyper graph partitioning third tier outperforming generalized means algorithms extended jaccard 
euclidean techniques including sofm performed poorly 
surprisingly sofm graph partitioning able signi cantly better random despite limited expressiveness euclidean similarity 
euclidean means performed worse random terms entropy equivalent random terms purity shown 
table shows results best performing opossum clustering strehl ghosh 
cluster dominant category purity entropy top descriptive discriminative words 
de ned occurrence frequency notion similar singular item set support 
discriminative terms cluster highest occurrence multipliers compared average document similar notion singular itemset lift 
yahoo categories vary size pages clusters balanced contains pages 
health kh turned category clearly identi ed 
expected language separates quite distinctively 
clustering better just matching yahoo labels distinguishes precisely 
example detected sub classes hiv genetics related pages 
cluster example described system terms strain indicating infection related cluster 
similarly entertainment people category algorithm identi es cluster dealing princess diana car accident 
smaller categories entertainment sub category pages ke absorbed meaningful clusters 
technology pages cluster interestingly documents technology category grouped entertainment online dominated business dominated cluster indicating overlap topics 
clusterings quality may build fully automated web page categorization engine yielding cleaner cut groups currently seen 
concluding remarks key contribution lies providing framework comparing clustering approaches variety similarity spaces 
results indicate graph partitioning better suited word frequency clustering web documents generalized means hyper graph partitioning sofm 
search procedure implicit graph partitioning far local hill climbing approach means 
provides way obtain balanced clusters exhibit lower variance results 
metric distances euclidean appropriate high dimensional sparse domains 
cosine correlation extended jaccard measures successful capturing similarities implicitly indicated manual categorizations seen example yahoo 
acknowledgments research supported aaai workshop arti cial intelligence web search july cu mu mu top descriptive terms abus label addict israel jet fda smoker lung weight breast diet strain hiv genet protein apple intel electron java sun miami fcc contract appeal republican reform smith coach marlin goal yard pass usa murdoch channel cent quarter dow greenspan rose canadian opera bing draw crash royal prince meredith classic spice radio station concert band stage south albert stone tour script sequel cast shoot tom theater writer script scot tom camera pic sound se winner king weekend gross household timeslot slot top discriminative terms grammer addict tractor vander stent latex fda insulin miner estrogen hiv gorman ibm compaq lucent java panama trump nato government reno homer marlin yard pearson gm dow greenspan stamp bing lange paperback stephan buckingham grief spencer burgess meredith fo solo cape calendar sequel bon cameron cast rep horse ira porsche quarter kim gross weekend monti timeslot cu mm mu table best clustering opossum extended jaccard similarity yahoo news pages 
cluster evaluations descriptive discriminative terms left confusion matrix right 
part nsf ecs 
dhillon helpful comments 
boley gini gross han hastings karypis kumar mobasher moore 
partitioning clustering web document categorization 
decision support systems 
craven dipasquo freitag mccallum mitchell nigam slattery 
learning extract symbolic knowledge world wide web 
aaai 
dhillon modha 
concept decompositions large sparse text data clustering 
technical report rj ibm almaden research center 
appear machine learning 
duda hart 
pattern classi cation scene analysis 
new york wiley 
frakes 
stemming algorithms 
frakes baeza yates eds information retrieval data structures algorithms 
new jersey prentice hall 

hartigan 
clustering algorithms 
new york wiley 
karypis kumar 
fast high quality multilevel scheme partitioning irregular graphs 
siam journal scienti computing 
karypis han kumar 
chameleon hierarchical clustering dynamic modeling 
ieee computer 
kohonen 
self organizing maps 
berlin heidelberg springer 
second extended edition 
kumar ghosh 
generalized framework associative modular learning systems 
proceedings applications science computational intelligence ii 
mooney roy 
content book recommending learning text categorization 
proceedings pf sigir workshop recommender systems algorithms evaluation 
rastogi shim 
scalable algorithms mining large databases 
han ed kdd tutorial notes 
acm 
strehl ghosh 
value customer grouping large retail data sets 
proc 
spie conference data mining knowledge discovery april orlando florida usa volume 
spie 
yang 
evaluation statistical approaches text categorization 
journal information retrieval 
